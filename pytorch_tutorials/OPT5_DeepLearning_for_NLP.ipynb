{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "OPT5_DeepLearning_for_NLP.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "[View in Colaboratory](https://colab.research.google.com/github/christopher-ell/Deep_Learning_Begin/blob/master/pytorch_tutorials/OPT5_DeepLearning_for_NLP.ipynb)"
      ]
    },
    {
      "metadata": {
        "id": "7VlQ5ULYbcGI",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Official Pytorch Tutorials - Deep Learning for NLP"
      ]
    },
    {
      "metadata": {
        "id": "wJtpv0oWixRZ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Source: https://pytorch.org/tutorials/beginner/deep_learning_nlp_tutorial.html"
      ]
    },
    {
      "metadata": {
        "id": "MCWgXFxPid7b",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        },
        "outputId": "b19af85d-536d-4cea-a611-80be1452b7ce"
      },
      "cell_type": "code",
      "source": [
        "## File created in Google colaboratory so need to download libraries and data on begin \n",
        "!pip install torch"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting torch\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/06/a7/6a173738dd6be014ebf9ba6f0b441d91b113b1506a98e10da4ff60994b54/torch-0.4.1-cp27-cp27mu-manylinux1_x86_64.whl (519.5MB)\n",
            "\u001b[K    100% |████████████████████████████████| 519.5MB 20kB/s \n",
            "tcmalloc: large alloc 1073750016 bytes == 0x55c99d38e000 @  0x7f5fdb8241c4 0x55c94396b0d8 0x55c943a54d5d 0x55c94397e77a 0x55c943983462 0x55c94397bb3a 0x55c94398382e 0x55c94397bb3a 0x55c94398382e 0x55c94397bb3a 0x55c94398382e 0x55c94397bb3a 0x55c943983e1f 0x55c94397bb3a 0x55c94398382e 0x55c94397bb3a 0x55c94398382e 0x55c943983462 0x55c943983462 0x55c94397bb3a 0x55c943983e1f 0x55c943983462 0x55c94397bb3a 0x55c943983e1f 0x55c94397bb3a 0x55c943983e1f 0x55c94397bb3a 0x55c94398382e 0x55c94397bb3a 0x55c9439ac50f 0x55c9439a7202\n",
            "\u001b[?25hInstalling collected packages: torch\n",
            "Successfully installed torch-0.4.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "SCteOt7yi_jI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "9a14605e-9a81-4bc2-b06f-7ffd676e22e0"
      },
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.autograd as autograd\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "torch.manual_seed(1)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7f201e372d30>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "metadata": {
        "id": "9MFG6wAOjI6J",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Creating Tensors**\n",
        "\n",
        "- Tensors can be created from Python lists with the torch.Tensor() function"
      ]
    },
    {
      "metadata": {
        "id": "LIVQfGtdjLQX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        },
        "outputId": "3dd41018-cfe2-461d-a8a0-6ebd43b41b3e"
      },
      "cell_type": "code",
      "source": [
        "# torch.tensor(data) creates a torch.Tensor object with the given data.\n",
        "V_data = [1., 2., 3.]\n",
        "V = torch.tensor(V_data)\n",
        "print(V)\n",
        "\n",
        "# Creates a Matrix\n",
        "M_data = [[1., 2., 3.], [4., 5., 6.]]\n",
        "M = torch.tensor(M_data)\n",
        "print(M)\n",
        "\n",
        "# Create a 3D tensor of size 2x2x2.\n",
        "T_data = [[[1., 2.], [3., 4.]],\n",
        "         [[5., 6.], [7., 8.]]]\n",
        "T = torch.tensor(T_data)\n",
        "print(T)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([1., 2., 3.])\n",
            "tensor([[1., 2., 3.],\n",
            "        [4., 5., 6.]])\n",
            "tensor([[[1., 2.],\n",
            "         [3., 4.]],\n",
            "\n",
            "        [[5., 6.],\n",
            "         [7., 8.]]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "7T4l7wjTmNJJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "06d0a150-0887-4ce3-f5a0-3feb6de6d1eb"
      },
      "cell_type": "code",
      "source": [
        "# Indexing into V and get a scalar (0 dimensional tensor)\n",
        "print(V[0])\n",
        "\n",
        "# Get a python number from it\n",
        "print(V[0].item())\n",
        "\n",
        "# Index into M and get a vector\n",
        "print(M[0])\n",
        "\n",
        "# Index into T and get a matrix\n",
        "print(T[0])"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor(1.)\n",
            "1.0\n",
            "tensor([1., 2., 3.])\n",
            "tensor([[1., 2.],\n",
            "        [3., 4.]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "rx-se2Rand_A",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        },
        "outputId": "8173176e-3800-427e-ec7f-86d1173cb1f9"
      },
      "cell_type": "code",
      "source": [
        "x = torch.randn((3, 4, 5))\n",
        "print(x)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[[-1.5256, -0.7502, -0.6540, -1.6095, -0.1002],\n",
            "         [-0.6092, -0.9798, -1.6091, -0.7121,  0.3037],\n",
            "         [-0.7773, -0.2515, -0.2223,  1.6871,  0.2284],\n",
            "         [ 0.4676, -0.6970, -1.1608,  0.6995,  0.1991]],\n",
            "\n",
            "        [[ 0.8657,  0.2444, -0.6629,  0.8073,  1.1017],\n",
            "         [-0.1759, -2.2456, -1.4465,  0.0612, -0.6177],\n",
            "         [-0.7981, -0.1316,  1.8793, -0.0721,  0.1578],\n",
            "         [-0.7735,  0.1991,  0.0457,  0.1530, -0.4757]],\n",
            "\n",
            "        [[-0.1110,  0.2927, -0.1578, -0.0288,  0.4533],\n",
            "         [ 1.1422,  0.2486, -1.7754, -0.0255, -1.0233],\n",
            "         [-0.5962, -1.0055,  0.4285,  1.4761, -1.7869],\n",
            "         [ 1.6103, -0.7040, -0.1853, -0.9962, -0.8313]]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "XGlEy1QSol8-",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Operations with Tensors**\n",
        "- You can operate on your own tensors in ways you could expect"
      ]
    },
    {
      "metadata": {
        "id": "R6UYowdoowj1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "e1a6014b-2324-4e57-ab7c-170f5b420d59"
      },
      "cell_type": "code",
      "source": [
        "x = torch.tensor([1., 2., 3.])\n",
        "y = torch.tensor([4., 5., 6.])\n",
        "z = x + y\n",
        "print(z)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([5., 7., 9.])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "vAn2lZSwpE-V",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        },
        "outputId": "e0dc6635-f23b-45b8-d19b-7becee82b3f8"
      },
      "cell_type": "code",
      "source": [
        "# By default, it concatenates along the first axis (concatinates rows)\n",
        "x_1 = torch.randn(2, 5)\n",
        "y_1 = torch.randn(3, 5)\n",
        "z_1 = torch.cat([x_1, y_1])\n",
        "print(z_1)\n",
        "\n",
        "# Concatenate columns:\n",
        "x_2 = torch.randn(2, 3)\n",
        "y_2 = torch.randn(2, 5)\n",
        "# Second arg specifies which axis to concat along\n",
        "z_2 = torch.cat([x_2, y_2], 1)\n",
        "print(z_2)\n",
        "\n",
        "# If your tensors are not compatible, torch will complain. Uncomment to see the \n",
        "# error \n",
        "# torch.cat([x_1, x_2])"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[-0.8029,  0.2366,  0.2857,  0.6898, -0.6331],\n",
            "        [ 0.8795, -0.6842,  0.4533,  0.2912, -0.8317],\n",
            "        [-0.5525,  0.6355, -0.3968, -0.6571, -1.6428],\n",
            "        [ 0.9803, -0.0421, -0.8206,  0.3133, -1.1352],\n",
            "        [ 0.3773, -0.2824, -2.5667, -1.4303,  0.5009]])\n",
            "tensor([[ 0.5438, -0.4057,  1.1341, -0.1473,  0.6272,  1.0935,  0.0939,  1.2381],\n",
            "        [-1.1115,  0.3501, -0.7703, -1.3459,  0.5119, -0.6933, -0.1668, -0.9999]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Pj9mSOt4q7wq",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Reshaping Tensors**\n",
        "- Use the .view() method to reshape tensors.\n",
        "- .view() is heavily used because many neural networks expect their inputs to have a certain shape"
      ]
    },
    {
      "metadata": {
        "id": "O4BYstxirV6Y",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        },
        "outputId": "24b2784e-2136-4ac8-c559-25136317cc09"
      },
      "cell_type": "code",
      "source": [
        "x = torch.randn(2, 3, 4)\n",
        "print(x)\n",
        "print(x.view(2, 12)) # Reshape to 2 rows, 12 columns\n",
        "# Same as above. If one of the dimensions is -1, it's size can be inferred\n",
        "print(x.view(2, -1))"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[[ 0.4175, -0.2127, -0.8400, -0.4200],\n",
            "         [-0.6240, -0.9773,  0.8748,  0.9873],\n",
            "         [-0.0594, -2.4919,  0.2423,  0.2883]],\n",
            "\n",
            "        [[-0.1095,  0.3126,  1.5038,  0.5038],\n",
            "         [ 0.6223, -0.4481, -0.2856,  0.3880],\n",
            "         [-1.1435, -0.6512, -0.1032,  0.6937]]])\n",
            "tensor([[ 0.4175, -0.2127, -0.8400, -0.4200, -0.6240, -0.9773,  0.8748,  0.9873,\n",
            "         -0.0594, -2.4919,  0.2423,  0.2883],\n",
            "        [-0.1095,  0.3126,  1.5038,  0.5038,  0.6223, -0.4481, -0.2856,  0.3880,\n",
            "         -1.1435, -0.6512, -0.1032,  0.6937]])\n",
            "tensor([[ 0.4175, -0.2127, -0.8400, -0.4200, -0.6240, -0.9773,  0.8748,  0.9873,\n",
            "         -0.0594, -2.4919,  0.2423,  0.2883],\n",
            "        [-0.1095,  0.3126,  1.5038,  0.5038,  0.6223, -0.4481, -0.2856,  0.3880,\n",
            "         -1.1435, -0.6512, -0.1032,  0.6937]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "6fdpdMB-s2JW",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Computation Graphs and Automatic Differentiation**\n",
        "- Computational graph important as it allows you to not have to write back propagation gradients yourself\n",
        "- A computational graph is a specification of how data is combined to give your output, so has enough information to compute derivatives\n",
        "- Can see what's going on using flag requires_grad\n",
        "\n",
        "- In the torch.tensor objects you have the data, shape and other things stored\n",
        "- When two tensors are added, we get an output tensor. All the output tensors knows is its data and shape. It has no idea it was the sum of the other two tensors\n",
        "- if requires_grad=True, the object keeps track of how it was created"
      ]
    },
    {
      "metadata": {
        "id": "vWytOphXunua",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "74bf5bd9-c2f2-4b67-e3f4-cda48f350262"
      },
      "cell_type": "code",
      "source": [
        "# Tensor factory method have a \"requires_grad\" flag\n",
        "x = torch.tensor([1., 2., 3.], requires_grad=True)\n",
        "\n",
        "# With requires_grad=True, you can still do all the operations you previously \n",
        "# could\n",
        "y = torch.tensor([4., 5., 6.], requires_grad=True)\n",
        "z = x + y\n",
        "print(z)\n",
        "\n",
        "# But z knows something extra\n",
        "print(z.grad_fn)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([5., 7., 9.], grad_fn=<ThAddBackward>)\n",
            "<ThAddBackward object at 0x7f1fed5292d0>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "LiOMVQyvv_dw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "805a55f3-875d-4c12-f21a-7e66844eaf35"
      },
      "cell_type": "code",
      "source": [
        "# Lets sum all the entries in z\n",
        "s = z.sum()\n",
        "print(s)\n",
        "print(s.grad_fn)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor(21., grad_fn=<SumBackward0>)\n",
            "<SumBackward0 object at 0x7f201e29e9d0>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "usTxVihgzZ2w",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "94ef83f9-1027-4fd7-8bb7-d926ac35de84"
      },
      "cell_type": "code",
      "source": [
        "# Calling .backward() on any variable will run backprop, starting from it.\n",
        "s.backward()\n",
        "print(x.grad)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([1., 1., 1.])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "o7ZKoEdK2bs7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "f47f79b6-2653-48f1-dfc1-dbcd6c9985b5"
      },
      "cell_type": "code",
      "source": [
        "x = torch.randn(2, 2)\n",
        "y = torch.randn(2, 2)\n",
        "# By default, user created Tensors have \"requires_grad=False\"\n",
        "print(x.requires_grad, y.requires_grad)\n",
        "z = x + y\n",
        "# So you can't backprop through z\n",
        "print(z.grad_fn)\n",
        "\n",
        "# \".requires_grad( ... )\" changes an existing Tensor's \"requires_grad\"\n",
        "# flag in place. The input flag defaults to \"True\" if not given.\n",
        "x = x.requires_grad_()\n",
        "y = y.requires_grad_()\n",
        "# z contains enough information to compute gradients, as we saw above\n",
        "z = x + y\n",
        "print(z.grad_fn)\n",
        "# If any input to an operation has \"requires_grad=True\", so will the output\n",
        "print(z.requires_grad)\n",
        "\n",
        "# Now z has the computation history that relates itself to x and y\n",
        "# Can we just take it's values, and **detach** it from its history\n",
        "new_z = z.detach()\n",
        "\n",
        "# ... does new_z have information to backprop to x and y?\n",
        "# NO!\n",
        "print(new_z.grad_fn)\n",
        "# and how could it? \"z.detach()\" returns a tensor that shares the same storage \n",
        "# as \"z\", but with the computation history forgotten. It doesn't know anything\n",
        "# about how it was computed.\n",
        "# In essence, we have broken the Tensor away from its past history."
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(False, False)\n",
            "None\n",
            "<ThAddBackward object at 0x7f1fed52f9d0>\n",
            "True\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "ocGo9CSV7B4C",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "ca56b3d4-5595-4ecb-d861-2ea36356de97"
      },
      "cell_type": "code",
      "source": [
        "print(x.requires_grad)\n",
        "print((x**2).requires_grad)\n",
        "\n",
        "with torch.no_grad():\n",
        "  print((x**2).requires_grad)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "True\n",
            "True\n",
            "False\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "x3dl5_Vik31f",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Deep Learning with Pytorch**\n",
        "- Deep learning consists of composing linearities with non-linearities\n",
        "- Affine maps (y = ax+b) are one of the workhorses of deep learning\n",
        "- Pytorch like other ML frameworks maps things by row rather than column (which linear algebra does)"
      ]
    },
    {
      "metadata": {
        "id": "k8G39Sxx7tUo",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "fd9247c4-9ebf-4ff7-8f49-d7ad5ac8e786"
      },
      "cell_type": "code",
      "source": [
        "lin = nn.Linear(5,3) # Maps from R^5 to R^3, parameters A, b\n",
        "# data is 2x5. A maps from 5 to 3... can we map data under A\n",
        "data = torch.randn(2, 5)\n",
        "print(data)\n",
        "print(lin(data)) # Yes"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[ 0.6684, -0.4101,  0.5338, -1.5819, -0.0689],\n",
            "        [-1.1353, -2.4166,  0.0554,  0.9053, -0.6205]])\n",
            "tensor([[-0.6831,  0.3639, -0.7709],\n",
            "        [ 0.6161,  1.2096, -0.3063]], grad_fn=<ThAddmmBackward>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "UCMEmgv4nvla",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Non-Linearities\n",
        "- Neural networks made up of multiple affine maps can be shown to simplify to a single affine map and so having multiple affine maps adds no new power\n",
        "- Introducing non-linearities makes much more powerful models\n",
        "- Some of the main non-linear maps are tan, sigma and ReLU, these functions have easy to compute gradients"
      ]
    },
    {
      "metadata": {
        "id": "z6edkPQck-i9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "1a97c153-b3e7-4e8f-e5df-0586b7972fd3"
      },
      "cell_type": "code",
      "source": [
        "# In pytorch most non-Linearities are in torch.functional (we have it imported as F)\n",
        "# Note non-linearities typically don't have parameters like affine maps do.\n",
        "# That is, they don't have weights that are updated by training.\n",
        "data = torch.randn(2,2)\n",
        "print(data)\n",
        "print(F.relu(data))"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[-0.1024, -0.8491],\n",
            "        [ 0.1112,  0.1618]])\n",
            "tensor([[0.0000, 0.0000],\n",
            "        [0.1112, 0.1618]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "zYpLpGbNpAJn",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Softmax and Probabilities\n",
        "- Softmax is often the last transform applied in a network as it takes a number and returns a probability distribution\n",
        "- It is given by the exponential of a number in the vector over the sum of the exponential of all the numbers in the vector for each number in the vector\n",
        "- The sum of all softmax values should be 1"
      ]
    },
    {
      "metadata": {
        "id": "kB6jLHMlneEJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "a93a0700-8023-4f7a-b8f9-6b8f86811e5a"
      },
      "cell_type": "code",
      "source": [
        "data = torch.randn(5)\n",
        "print(data)\n",
        "print(F.softmax(data, dim=0))\n",
        "print(F.softmax(data, dim=0).sum()) # Sums to 1 because it is a distribution\n",
        "print(F.log_softmax(data, dim=0)) # Theres also log_softmax"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([-1.4105, -0.3404, -3.0121,  0.5710,  1.4330])\n",
            "tensor([0.0350, 0.1021, 0.0071, 0.2541, 0.6017])\n",
            "tensor(1.)\n",
            "tensor([-3.3515, -2.2815, -4.9531, -1.3700, -0.5080])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "CZPreoEMqFDu",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Objective functions\n",
        "- Also known as loss function, it is the function your model is being trained to minimise\n",
        "- To get a loss your model is run with a set of inputs and then the difference between the model output and the actual result is run through the loss function\n",
        "- The model parameters are then updated taking into account the gradient of the loss function\n",
        "- By minimising the loss function by altering the models parameters the model will hopefully generalise well and so work well on unseen examples\n",
        "\n",
        "Optimisation and Training\n",
        "- After computing a single loss instance wrt the model parameters, the parameters can then be updated using the loss gradient\n",
        "- There are a range of packages designed to improve the speed of the gradient update (gradient descent) in torch.optim(), it is also an area of curent active research\n",
        "\n",
        "Creating Network Components in Pytorch\n",
        "- All network components should inherit the nn.Module and overide the forward method\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "8DblPyNZqESU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 258
        },
        "outputId": "71afa341-9092-45ab-f1f5-f51ae99325ec"
      },
      "cell_type": "code",
      "source": [
        "## Example Bag of Words (BoW)\n",
        "\n",
        "data = [(\"me gusta comer en la cafeteria\".split(), \"SPANISH\"),\n",
        "       (\"Give it to me\".split(), \"ENGLISH\"),\n",
        "       (\"No creo que sea una buena idea\".split(), \"SPANISH\"),\n",
        "       (\"No it is not a good idea to get lost at sea\".split(), \"ENGLISH\")]\n",
        "\n",
        "test_data = [(\"Yo creo que si\".split(), \"SPANISH\"),\n",
        "            (\"it is lost on me\".split(), \"ENGLISH\")]\n",
        "\n",
        "# word_to_ix maps each word in the vocab to a unique integer, which will be its\n",
        "# index into the bag of words vector\n",
        "word_to_ix = {}\n",
        "for sent, _ in data + test_data:\n",
        "  for word in sent:\n",
        "    if word not in word_to_ix:\n",
        "      word_to_ix[word] = len(word_to_ix)\n",
        "\n",
        "print(word_to_ix)\n",
        "\n",
        "VOCAB_SIZE = len(word_to_ix)\n",
        "NUM_LABELS = 2\n",
        "\n",
        "\n",
        "class BoWClassifier(nn.Module):   # inheriting from nn.Module\n",
        "  \n",
        "  def __init__(self, num_labels, vocab_size):\n",
        "    # Calls the init function of nn.Module. Dont get confused by syntax,\n",
        "    # just always do it in an nn.Module\n",
        "    super(BoWClassifier, self).__init__()\n",
        "    \n",
        "    # Define the parameters that you will need. In this case, we need A and b,\n",
        "    # the parameters of the affine mapping.\n",
        "    # Torch defines nn.Linear(), which provides the affine map.\n",
        "    # Make sure you understand why the input dimension is vocab_size\n",
        "    # and the output is num_labels!\n",
        "    self.linear = nn.Linear(vocab_size, num_labels)\n",
        "    \n",
        "    # NOTE: The non-linearity log softmax does not have parameters! So we don't need \n",
        "    # to worry about that here. \n",
        "    \n",
        "  def forward(self, bow_vec):\n",
        "    # Pass the input through the linear layer, \n",
        "    # then pass that through log_softmax.\n",
        "    # Many non-linearities and other functions are in torch.nn.functional\n",
        "    return F.log_softmax(self.linear(bow_vec), dim=1)\n",
        "    \n",
        "def make_bow_vector(sentence, word_to_ix):\n",
        "  vec = torch.zeros(len(word_to_ix))\n",
        "  for word in sentence:\n",
        "    vec[word_to_ix[word]] += 1\n",
        "  return vec.view(1, -1)\n",
        "    \n",
        "def make_target(labels, label_to_ix):\n",
        "  return torch.LongTensor([label_to_ix[label]])\n",
        "    \n",
        "model = BoWClassifier(NUM_LABELS, VOCAB_SIZE)\n",
        "\n",
        "# The model knows its parameters. The first output below is A, the second is b.\n",
        "# Whenever you assign a component to a class variable in the __init__ function\n",
        "# of a module, which was done with the line.\n",
        "# Then through some Python magic from the Pytorch devs, your module\n",
        "# (in this case, BoWClassifier) will store knowledge of the nn.linear's parameters\n",
        "for param in model.parameters():\n",
        "  print(param)\n",
        "  \n",
        "# To run the model, pass in the BoW vector\n",
        "# Here we don't need to train, so the code is wrapped in torch.no_grad()\n",
        "with torch.no_grad():\n",
        "  sample = data[0]\n",
        "  bow_vector = make_bow_vector(sample[0], word_to_ix)\n",
        "  log_probs = model(bow_vector)\n",
        "  print(log_probs)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'en': 3, 'No': 9, 'buena': 14, 'it': 7, 'at': 22, 'sea': 12, 'cafeteria': 5, 'Yo': 23, 'la': 4, 'to': 8, 'creo': 10, 'is': 16, 'a': 18, 'good': 19, 'get': 20, 'idea': 15, 'que': 11, 'not': 17, 'me': 0, 'on': 25, 'gusta': 1, 'lost': 21, 'Give': 6, 'una': 13, 'si': 24, 'comer': 2}\n",
            "Parameter containing:\n",
            "tensor([[ 0.0555,  0.0597,  0.0466,  0.1627, -0.0815, -0.0828, -0.1699, -0.0080,\n",
            "         -0.0929,  0.0079, -0.0402,  0.0651,  0.1697,  0.0579, -0.0632, -0.0962,\n",
            "         -0.1710,  0.1650, -0.0372,  0.0396,  0.0073, -0.1250,  0.1104,  0.1099,\n",
            "          0.0099, -0.1115],\n",
            "        [-0.0833,  0.0027, -0.1120, -0.1094, -0.0293, -0.0565,  0.0481, -0.0515,\n",
            "         -0.0260, -0.0749, -0.1792,  0.1710,  0.0374,  0.1754, -0.0316, -0.0493,\n",
            "         -0.1844, -0.0744,  0.1286, -0.1921, -0.0686,  0.1195,  0.1130,  0.0724,\n",
            "         -0.0388, -0.0148]], requires_grad=True)\n",
            "Parameter containing:\n",
            "tensor([-0.0372, -0.0723], requires_grad=True)\n",
            "tensor([[-0.4435, -1.0266]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "5NLZnNGxxooJ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "label_to_ix = {\"SPANISH\": 0, \"ENGLISH\": 1}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "wrHalaEab2aR",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Training\n",
        "1. Randomly generate parameters to start\n",
        "2. Run the model with the parameters and some examples.\n",
        "3. Compute the difference between the model output with the actual results and run this through the loss function. In this case the Negative Log Likelihood Loss (NLLLoss)\n",
        "4. Compute the gradient of the loss function\n",
        "5. Update parameters using the gradient of the loss function as the direction and magnitude of the change\n",
        "6. Repeat steps 2-5 for whole dataset for each epoch"
      ]
    },
    {
      "metadata": {
        "id": "MvSB4iUnkLBS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "0d80613f-3033-4b6a-877f-4fddd4d4b1b6"
      },
      "cell_type": "code",
      "source": [
        "# Run on test data before we train, just to see a before-and-after\n",
        "with torch.no_grad():\n",
        "  for instance, label in test_data:\n",
        "    bow_vec = make_bow_vector(instance, word_to_ix)\n",
        "    log_probs = model(bow_vec)\n",
        "    print(\"Test Data log_probs prior to Training: \", log_probs)\n",
        "    \n",
        "# Print the matrix column corresponding to \"creo\"\n",
        "print(\"Parameters for 'CREO' prior to training: \", next(model.parameters())[:, word_to_ix[\"creo\"]])\n",
        "\n",
        "loss_function = nn.NLLLoss()\n",
        "optimiser = optim.SGD(model.parameters(), lr=0.1)\n",
        "\n",
        "# Usually you want to pass over the training data several times.\n",
        "# 100 is much bigger than on a real data set, but real datasets have more than\n",
        "# two instances. Usually, somewhere between 5 and 30 epochs is reasonable.\n",
        "for epoch in range(100):\n",
        "  for instance, label in data:\n",
        "    # Step 1: Remember that Pytorch accumulates gradients.\n",
        "    # We need to clear them out before each instance \n",
        "    model.zero_grad()\n",
        "    \n",
        "    # Step 2: Make our BOW vector and also we must wrap the target in a\n",
        "    # tensor as an integer. For example, if the target is SPANISH, then\n",
        "    # we wrap the integer 0. The loss function then knows that the 0th\n",
        "    # element of the log probabilities is the log probabilities \n",
        "    # corresponding to SPANISH\n",
        "    bow_vec = make_bow_vector(instance, word_to_ix)\n",
        "    target = make_target(label, label_to_ix)\n",
        "    \n",
        "    # Step 3: Run our forward pass\n",
        "    log_probs = model(bow_vec)\n",
        "    \n",
        "    # Step 4: Compute the loss, gradients, and update the parameters by\n",
        "    # calling optimiser.step()\n",
        "    loss = loss_function(log_probs, target)\n",
        "    loss.backward()\n",
        "    optimiser.step()\n",
        "\n",
        "with torch.no_grad():\n",
        "  for instance, label in test_data:\n",
        "    bow_vec = make_bow_vector(instance, word_to_ix)\n",
        "    log_probs = model(bow_vec)\n",
        "    print(\"Test Data log_probs after Training: \", log_probs)\n",
        "    \n",
        "# Index corresponding to Spanish goes up, English goes down!\n",
        "print(\"Parameters for 'CREO' after training: \", next(model.parameters())[:, word_to_ix[\"creo\"]])"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "('Test Data log_probs prior to Training: ', tensor([[-0.6190, -0.7733]]))\n",
            "('Test Data log_probs prior to Training: ', tensor([[-0.7499, -0.6395]]))\n",
            "(\"Parameters for 'CREO' prior to training: \", tensor([-0.0402, -0.1792], grad_fn=<SelectBackward>))\n",
            "('Test Data log_probs after Training: ', tensor([[-0.1241, -2.1481]]))\n",
            "('Test Data log_probs after Training: ', tensor([[-2.9001, -0.0566]]))\n",
            "(\"Parameters for 'CREO' after training: \", tensor([ 0.3966, -0.6160], grad_fn=<SelectBackward>))\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "-6nbpBMdSMaN",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Word Embeddings: Encoding Lexical Semantics**\n",
        "- In NLP it is almost always the case that your words are your features, but how should you represent a word in a computer?\n",
        "- One way of representing words is \"one hot encoding\" where each word is represented by its own vector equal to the size of the text with a 1 where the word appears and a 0 elsewhere\n",
        "- One hot encoding has two draw backs. It creates huge vectors with sparse amounts of information and it treats each word as being independent\n",
        "\n",
        "Getting Dense Word Embeddings\n",
        "- So how can we encode semantically similar words?\n",
        "- One of the central ideas to deep learning is that neural networks learn representations of features rather than the programmer designing them themselves.\n",
        "- So instead we will let the word embeddings be parameters in our model that are updated during training\n",
        "- In doing so we will have latent semantic attributes that the model will learn, but likely be uninterpretable\n",
        "- In summary: Word embeddings are a summary of the \"semantics\" of a word, efficiently encoding information that might be relevant to the task at hand\n",
        "\n",
        "Word Embeddings in Pytorch\n",
        "- We need to define a unique index for each word when using embeddings, defining them as keys in a lookup table\n",
        "- The matrix will have rows equal to the number of words and columns equal to the number of \"latent attributes,\" which initially is an arbitrary number but can be thought of as a hyperparameter.\n"
      ]
    },
    {
      "metadata": {
        "id": "gc3LrB4QSU9y",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "efb2d3c1-711e-483e-ca14-5ad36ce11d10"
      },
      "cell_type": "code",
      "source": [
        "word_to_ix = {\"hello\": 0, \"world\": 1}\n",
        "embeds = nn.Embedding(2, 5) #2 words in vocab, 5 dimensional embeddings\n",
        "lookup_tensor = torch.tensor([word_to_ix[\"hello\"]], dtype=torch.long)\n",
        "hello_embed = embeds(lookup_tensor)\n",
        "print(hello_embed)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[-0.8120, -1.4617,  0.2328,  0.1896, -0.2204]], grad_fn=<EmbeddingBackward>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "P42T6pcdnoFM",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "An Example: N-Gram Language Modelling\n",
        "- N-Gram model gives a sequence of words we want to compute"
      ]
    },
    {
      "metadata": {
        "id": "8Xxpqm9-W04-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "0b0743fa-a009-49b7-8dea-8930cc5c0ce2"
      },
      "cell_type": "code",
      "source": [
        "CONTEXT_SIZE = 2\n",
        "EMBEDDING_DIM = 10\n",
        "# We will use Shakespeare Sonnet 2\n",
        "test_sentence = \"\"\"When forty winters shall besiege thy brow,\n",
        "And dig deep trenches in thy beauty's field,\n",
        "Thy youth's proud livery so gazed on now,\n",
        "Will be a totter'd weed of small worth held:\n",
        "Then being asked, where all thy beauty lies,\n",
        "Where all the treasure of thy lusty days;\n",
        "To say, within thine own deep sunken eyes,\n",
        "Were an all-eating shame, and thriftless praise.\n",
        "How much more praise deserv'd thy beauty's use,\n",
        "If thou couldst answer 'This fair child of mine\n",
        "Shall sum my count, and make my old excuse,'\n",
        "Proving his beauty by succession thine!\n",
        "This were to be new made when thou art old,\n",
        "And see thy blood warm when thou feel'st it cold.\"\"\".split()\n",
        "\n",
        "# We should tokenize the input, but we will ignore that for now\n",
        "# build a list of tuples. Each tuple is ([word_i-2, word_i-1], target word)\n",
        "trigrams = [([test_sentence[i], test_sentence[i + 1]], test_sentence[i + 2])\n",
        "           for i in range(len(test_sentence) - 2)]\n",
        "# print the first 3, just so you can see what they look like\n",
        "print(trigrams[:3])\n",
        "\n",
        "vocab = set(test_sentence)\n",
        "word_to_ix = {word: i for i, word in enumerate(vocab)}\n",
        "\n",
        "class NGramLanguageModeler(nn.Module):\n",
        "  \n",
        "  def __init__(self, vocab_size, embedding_dim, context_size):\n",
        "    super(NGramLanguageModeler, self).__init__()\n",
        "    self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
        "    self.linear1 = nn.Linear(context_size * embedding_dim, 128)\n",
        "    self.linear2 = nn.Linear(128, vocab_size)\n",
        "    \n",
        "  def forward(self, inputs):\n",
        "    embeds = self.embeddings(inputs).view((1, -1))\n",
        "    out = F.relu(self.linear1(embeds))\n",
        "    out = self.linear2(out)\n",
        "    log_probs = F.log_softmax(out, dim=1)\n",
        "    return log_probs\n",
        "\n",
        "losses = []\n",
        "loss_function = nn.NLLLoss()\n",
        "model = NGramLanguageModeler(len(vocab), EMBEDDING_DIM, CONTEXT_SIZE)\n",
        "optimiser = optim.SGD(model.parameters(), lr=0.001)\n",
        "\n",
        "for epoch in range(10):\n",
        "  total_loss = 0\n",
        "  for context, target in trigrams:\n",
        "    \n",
        "    # Step 1. Prepare the inputs to be passed to the model (i.e. turn the words\n",
        "    # into integer indicies and wrap them in tensors)\n",
        "    context_idxs = torch.tensor([word_to_ix[w] for w in context], dtype=torch.long)\n",
        "    \n",
        "    # Step 2. Recall that torch *accumulates* gradients. Before passing in a \n",
        "    # new instance, you need to zero out the gradients from the old \n",
        "    # instance.\n",
        "    model.zero_grad()\n",
        "    \n",
        "    # Step 3. Run the forward pass, getting log probabilities over next\n",
        "    # words\n",
        "    log_probs = model(context_idxs)\n",
        "    \n",
        "    # Step 4. Compute your loss function (Again, torch wants the target \n",
        "    # word wrapped in a tensor)\n",
        "    loss = loss_function(log_probs, torch.tensor([word_to_ix[target]], dtype=torch.long))\n",
        "    \n",
        "    # Step 5. Do the backward pass and update the gradient\n",
        "    loss.backward()\n",
        "    optimiser.step()\n",
        "    \n",
        "    # Get the Python number from a 1-element tensor by calling tensor.item()\n",
        "    total_loss += loss.item()\n",
        "  losses.append(total_loss)\n",
        "print(losses) # The loss decreased every iteration over the training data."
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[(['When', 'forty'], 'winters'), (['forty', 'winters'], 'shall'), (['winters', 'shall'], 'besiege')]\n",
            "[520.4770035743713, 518.228696346283, 515.9942712783813, 513.771467924118, 511.56114411354065, 509.36237955093384, 507.17383098602295, 504.99530148506165, 502.82693791389465, 500.6679017543793]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "sAlRY17erbKm",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Exercise: Computing Word Embeddings: Continuous Bag-of-Words\n",
        "- Tries to predict words being given a few words before and a few after.\n",
        "- Typically used to quickly train word embeddings that are used to initialialise embeddings of more complicated models (pretraining embeddings)"
      ]
    },
    {
      "metadata": {
        "id": "O1xELiMi2imM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "2fdeb68d-1746-4c49-ecd2-ba48778fa039"
      },
      "cell_type": "code",
      "source": [
        "## NEED TO FINISH EXERCISE\n",
        "\n",
        "CONTEXT_SIZE = 2 # 2 words to the left and 2 words to the right\n",
        "raw_text = \"\"\"We are about to study the idea of a computational process.\n",
        "Computational processes are abstract beings that inhabit computers.\n",
        "As they evolve, processes manipulate other abstract things called data.\n",
        "The evolution of a process is directed by a pattern of rules\n",
        "called a program. People create programs to direct processes. In effect,\n",
        "we conjure the spirits of the computer with our spells.\"\"\".split()\n",
        "\n",
        "# By deriving a set from \"raw_text,\" we deduplicate the array\n",
        "vocab = set(raw_text)\n",
        "vocab_size = len(vocab)\n",
        "\n",
        "word_to_ix = {word: i for i, word in enumerate(vocab)}\n",
        "data = []\n",
        "for i in range(2, len(raw_text) - 2):\n",
        "  context = [raw_text[i - 2], raw_text[i - 1], \n",
        "             raw_text[i + 1], raw_text[i + 2]]\n",
        "  target = raw_text[i]\n",
        "  data.append((context, target))\n",
        "print(data[:5])\n",
        "\n",
        "class CBOW(nn.Module):\n",
        "  \n",
        "  def __init__(self):\n",
        "    pass\n",
        "  \n",
        "  def forward(self, inputs):\n",
        "    pass\n",
        "  \n",
        "  \n",
        "# Create the model and train. Here are some functions to make the data ready \n",
        "# for use by your module.\n",
        "\n",
        "def make_context_vector(context, word_to_ix):\n",
        "  idxs = [word_to_ix[w] for w in context]\n",
        "  return torch.tensor(idxs, dtype=torch.long)\n",
        "\n",
        "make_context_vector(data[0][0], word_to_ix) # example\n",
        "  "
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[(['We', 'are', 'to', 'study'], 'about'), (['are', 'about', 'study', 'the'], 'to'), (['about', 'to', 'the', 'idea'], 'study'), (['to', 'study', 'idea', 'of'], 'the'), (['study', 'the', 'of', 'a'], 'idea')]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([13,  7, 18, 42])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "metadata": {
        "id": "Dx2fYYQvV15V",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Sequence Models and Long-Short Term Memory Models**\n",
        "- So far only looked at feed forward networks\n",
        "- Sequence models differ as there is a dependence through time between your inputs e.g. Hdden Markov Models\n",
        "- A Recurrent Neural Network (RNN) maintains a state e.g. Its output could be used as part of its input so that information can propagate as the network passes over the sequence\n",
        "- In an LSTM there is a hidden state (h), which can contain information from arbitrary points earlier in the sequence.\n",
        "- The hidden state can be used to predict words in a language model \n",
        "\n",
        "LSTM's in Pytorch\n",
        "- Expects all inputs to be 3D\n",
        "  1. First axis is sequence itself\n",
        "  2. Second axis indexes instances of the mini-batch\n",
        "  3. Indexes elements of the input"
      ]
    },
    {
      "metadata": {
        "id": "3RXwbjmIV9f9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        },
        "outputId": "f0d60b3a-918a-4454-d7f7-656ebef2157e"
      },
      "cell_type": "code",
      "source": [
        "lstm = nn.LSTM(3, 3) # Input dim is 3, output dim is 3\n",
        "inputs = [torch.randn(1, 3) for _ in range(5)] # Make a sequence of length 5\n",
        "\n",
        "# Initialise the hidden state.\n",
        "hidden = (torch.randn(1, 1, 3),\n",
        "          torch.randn(1, 1, 3))\n",
        "for i in inputs:\n",
        "  # Step through the sequence one element at a time\n",
        "  # after each step, hidden contains the hidden state\n",
        "  out, hidden = lstm(i.view(1, 1, -1), hidden)\n",
        "  \n",
        "# Alternatively, we can do the entire sequence all at once.\n",
        "# the first value returned by lstm is all of the hidden states throughout\n",
        "# the sequnce. The second is just the most recent hidden state\n",
        "# (compare the last slice of \"out\" with \"hidden\" below, they are the same)\n",
        "# The reason for that is that:\n",
        "# \"out\" will; give you access to all hidden states in a sequence\n",
        "# hidden will allow you to continue the sequence and backpropagate,\n",
        "# by passing it as an argument to the lstm at a later time\n",
        "# Add the extra second dimension\n",
        "inputs = torch.cat(inputs).view(len(inputs), 1, -1)\n",
        "hidden = (torch.randn(1, 1, 3), torch.randn(1, 1, 3)) # clean out hidden state\n",
        "out, hidden = lstm(inputs, hidden)\n",
        "print(out)\n",
        "print(hidden)"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[[-0.0955, -0.2526, -0.2748]],\n",
            "\n",
            "        [[-0.3970, -0.0469, -0.1467]],\n",
            "\n",
            "        [[-0.1144, -0.0950, -0.1793]],\n",
            "\n",
            "        [[-0.0543,  0.0513, -0.1117]],\n",
            "\n",
            "        [[-0.2153,  0.0141, -0.1913]]], grad_fn=<CatBackward>)\n",
            "(tensor([[[-0.2153,  0.0141, -0.1913]]], grad_fn=<ViewBackward>), tensor([[[-0.5291,  0.0546, -0.3974]]], grad_fn=<ViewBackward>))\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "A653j6_TUHus",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Example: LSTM for Part-of-Speech Tagging"
      ]
    },
    {
      "metadata": {
        "id": "bQdVUBVcjZ4X",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "b6760e83-ba29-4a95-931f-4b7f970b81c1"
      },
      "cell_type": "code",
      "source": [
        "## Prepare Data\n",
        "\n",
        "def prepare_sequence(seq, to_ix):\n",
        "  idxs = [to_ix[w] for w in seq]\n",
        "  return torch.tensor(idxs, dtype=torch.long)\n",
        "\n",
        "training_data = [\n",
        "    (\"The dog ate the apple\".split(), [\"DET\", \"NN\", \"V\", \"DET\", \"NN\"]),\n",
        "    (\"Everybody read the book\".split(), [\"NN\", \"DET\", \"V\", \"NN\"])\n",
        "]\n",
        "\n",
        "word_to_ix={}\n",
        "for sent, tags in training_data:\n",
        "  for word in sent:\n",
        "    if word not in word_to_ix:\n",
        "      word_to_ix[word] = len(word_to_ix)\n",
        "print(word_to_ix)\n",
        "tag_to_ix = {\"DET\":0, \"NN\":1, \"V\":2}\n",
        "\n",
        "# These will usually be more like 32 or 64 dimensional\n",
        "# We will keep them small, so we can see how the weights change as we train.\n",
        "EMBEDDING_DIM = 6\n",
        "HIDDEN_DIM = 6"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'Everybody': 5, 'ate': 2, 'apple': 4, 'read': 6, 'dog': 1, 'book': 7, 'the': 3, 'The': 0}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "0B51jHR7WoTD",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "## Create The Model\n",
        "\n",
        "class LSTMTagger(nn.Module):\n",
        "  def __init__(self, embedding_dim, hidden_dim, vocab_size, tagset_size):\n",
        "    super(LSTMTagger, self).__init__()\n",
        "    self.hidden_dim = hidden_dim\n",
        "  \n",
        "    self.word_embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
        "  \n",
        "    # The LSTM takes word embeddings as inputs, and outputs hidden states\n",
        "    # with dimensionality hidden dim.\n",
        "    self.lstm = nn.LSTM(embedding_dim, hidden_dim)\n",
        "  \n",
        "    # The linear layer that maps from hidden state space to tagspace\n",
        "    self.hidden2tag = nn.Linear(hidden_dim, tagset_size)\n",
        "    self.hidden = self.init_hidden()\n",
        "  \n",
        "  def init_hidden(self):\n",
        "    # Before we've done anything, we don't have any hidden state.\n",
        "    # Refer to the Pytorch documentation to see exactly\n",
        "    # why they have this dimensionality\n",
        "    # The axes semantics are (num_layers, minibatch_size, hidden_dim)\n",
        "    return (torch.zeros(1, 1, self.hidden_dim), \n",
        "            torch.zeros(1, 1, self.hidden_dim))\n",
        "\n",
        "  def forward(self, sentence):\n",
        "    embeds = self.word_embeddings(sentence)\n",
        "    lstm_out, self.hidden = self.lstm(\n",
        "        embeds.view(len(sentence), 1, -1), self.hidden)\n",
        "    tag_space = self.hidden2tag(lstm_out.view(len(sentence), -1))\n",
        "    tag_scores = F.log_softmax(tag_space, dim=1)\n",
        "    return tag_scores"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Edyjs45LXiJH",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Train The Model"
      ]
    },
    {
      "metadata": {
        "id": "5S71j1j9Z-YQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        },
        "outputId": "16324a5b-5c32-4d0b-d0e3-10e3cc65457b"
      },
      "cell_type": "code",
      "source": [
        "model = LSTMTagger(EMBEDDING_DIM, HIDDEN_DIM, len(word_to_ix), len(tag_to_ix))\n",
        "loss_function = nn.NLLLoss()\n",
        "optimiser = optim.SGD(model.parameters(), lr=0.1)\n",
        "\n",
        "# See what the scores are before training\n",
        "# that element i, j of the output is the score for tag j for word i.\n",
        "# Here we don't need to train, so the code is wrapped in torch.no_grad()\n",
        "with torch.no_grad():\n",
        "  inputs = prepare_sequence(training_data[0][0], word_to_ix)\n",
        "  tag_scores = model(inputs)\n",
        "  print(tag_scores)\n",
        "  \n",
        "for epoch in range(300): # Again normally you would not do 300 epochs, it is toy data\n",
        "  for sentence, tags in training_data:\n",
        "    # Step 1. Remember that Pytorch accumulates gradients.\n",
        "    # we need to clear them out before each instance\n",
        "    model.zero_grad()\n",
        "    \n",
        "    # Also we need to clear out the hidden state of the LSTM,\n",
        "    # detaching it from its history on its last instance.\n",
        "    model.hidden = model.init_hidden()\n",
        "    \n",
        "    # Step 2. Get our inputs ready for the network, that is, turn them into \n",
        "    # Tensors of word indicies.\n",
        "    sentence_in = prepare_sequence(sentence, word_to_ix)\n",
        "    targets = prepare_sequence(tags, tag_to_ix)\n",
        "    \n",
        "    # Step 3. Run our forward pass\n",
        "    tag_scores = model(sentence_in)\n",
        "    \n",
        "    # Step 4. Compute the loss, gradient and update the parameters by \n",
        "    # calling optimiser.step()\n",
        "    loss = loss_function(tag_scores, targets)\n",
        "    loss.backward()\n",
        "    optimiser.step()\n",
        "    \n",
        "# See what the scores are after training\n",
        "with torch.no_grad():\n",
        "  inputs = prepare_sequence(training_data[0][0], word_to_ix)\n",
        "  tag_scores = model(inputs)\n",
        "  \n",
        "  # The sentence is \"The dog ate the apple\". i, j corresponding to score for tag j\n",
        "  # for word i. The predicted tag is the maximum scoring tag.\n",
        "  # Here, we can see the predicted sequence below is 0 1 2 0 1\n",
        "  # since 0is index of the maximum value of 1,\n",
        "  # 1 is the index of the maxiumum value of row 2, etc. \n",
        "  # Which is DET NOUN VERB DET NOUN, the correct sequence\n",
        "  print(tag_scores)  "
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[-1.5599, -1.1269, -0.7640],\n",
            "        [-1.4766, -1.1624, -0.7790],\n",
            "        [-1.4501, -1.2148, -0.7579],\n",
            "        [-1.4652, -1.1583, -0.7876],\n",
            "        [-1.3721, -1.1398, -0.8520]])\n",
            "tensor([[-0.3886, -1.1934, -3.9739],\n",
            "        [-4.5664, -0.0473, -3.3296],\n",
            "        [-2.7397, -2.3611, -0.1731],\n",
            "        [-0.2229, -3.6717, -1.7466],\n",
            "        [-4.7634, -0.0145, -5.1416]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "EwjSoycKXkDl",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Helper functions to make the code more readable\n",
        "\n",
        "def argmax(vec):\n",
        "  # Return the argmax as a Python int\n",
        "  _, idx = torch.max(vec, 1)\n",
        "  return idx.item()\n",
        "\n",
        "def prepare_sequence(seq, to_idx):\n",
        "  idxs = [to_ix[w] for w in seq]\n",
        "  return torch.tensor(idxs, dtype=torch.long)\n",
        "\n",
        "# Compute log sum exp in a numercially stable wayfor the forward algorithm\n",
        "def log_sum_exp(vec):\n",
        "  max_score = vec[0, argmax(vec)]\n",
        "  max_score_broadcast = max_score.view(1, -1).expand(1, vec.size()[1])\n",
        "  return max_score + \\\n",
        "        torch.log(torch.sum(torch.exp(vec - max_score_broadcast)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "1qfyLDXdievq",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class BiLSTM_CRF(nn.Module):\n",
        "  \n",
        "  def __init__(self, vocab_size, tag_to_ix, embedding_dim, hidden_dim):\n",
        "    super(BiLSTM_CRF, self).__init__()\n",
        "    self.embedding_dim = embedding_dim\n",
        "    self.hidden_dim = hidden_dim\n",
        "    self.vocab_size = vocab_size\n",
        "    self.tag_to_ix = tag_to_ix\n",
        "    self.tagset_size = len(tag_to_ix)\n",
        "    \n",
        "    self.word_embeds = nn.Embedding(vocab_size, embedding_dim)\n",
        "    self.lstm = nn.LSTM(embedding_dim, hidden_dim // 2, \n",
        "                       num_layers=1, bidirectional=True)\n",
        "    \n",
        "    # Maps the output of the LSTM into tag space.\n",
        "    self.hidden2tag = nn.linear(hidden_dim, self.tagset_size)\n",
        "    \n",
        "    # Matrix of transition parameters. Entry i, j is the score of \n",
        "    # transitioning *to* i *from* j.\n",
        "    self.transitions = nn.Parameter(\n",
        "      torch.randn(self.tagset_size, self.tagset_size))\n",
        "    \n",
        "    # These two statementss enforce the constraint that we never transfer\n",
        "    # to the start tag and we never transfer from the stop tag.\n",
        "    self.transitions.data[tag_to_ix[START_TAG], :] = -10000\n",
        "    self.transitions.data[:, tag_to_ix[STOP_TAG]] = -10000\n",
        "    \n",
        "    self.hidden = self.init_hidden()\n",
        "    \n",
        "  def init_hidden(self):\n",
        "    return (torch.randn(2, 1, self.hidden_dim // 2),\n",
        "           torch.randn(2, 1, self.hidden_dim // 2))\n",
        "  \n",
        "  def _forward_alg(self, feats):\n",
        "    # Do the forward alforithm to compute the partition function\n",
        "    init_alphas = torch.full((1, self.target_size), -10000.)\n",
        "    # START_TAG has all of the score\n",
        "    init_alphas[0][self.tag_to_ix[START_TAG]] = 0.\n",
        "    \n",
        "    # wrap in a variable so that we will get automatic backprop\n",
        "    forward_var = init_alphas\n",
        "    \n",
        "    # Iterate through the sentence\n",
        "    for feat in feats:\n",
        "      alphas_t = [] # The forward tensors at this timestep\n",
        "      for next_tag in range(self.target_size):\n",
        "        # Broadcast the emission score: it is the same regardless of \n",
        "        # the previous tag\n",
        "        emit_score = feat[next_tag].view(\n",
        "          1, -1).expand(1, self.target_size)\n",
        "        # The ith entry of trans_score is the score of transitioning to \n",
        "        # the next_tag from i\n",
        "        trans_score = self.transitions[next_tag].view(1, -1)\n",
        "        # The ith entry of next_tag_var is the value for the \n",
        "        # edge (i -> next tag) before we do log-sum-exp\n",
        "        next_tag_var = forward_var + trans_score + emit_score\n",
        "        # The forward variable for this tag is log-sum-exp of all the \n",
        "        # scores.\n",
        "        alpha_t.append(log_sum_exp(next_tag_var).view(1, -1))\n",
        "      forward_var = torch.cat(alphas_t).view(1, -1)\n",
        "    terminal_var = forward_var + self.transitions[self.tag_to_ix[STOP_TAG]]\n",
        "    alpha = log_sum_exp(terminal_var)\n",
        "    return alpha\n",
        "  \n",
        "  def _get_lstm_features(self, sentence):\n",
        "    self.hidden = self.init_hidden()\n",
        "    embeds = self.word_embeds(sentence).view(len(sentence), 1, -1)\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "48xsewwzUwHT",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}