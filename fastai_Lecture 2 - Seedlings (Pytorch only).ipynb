{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function, division\n",
    "import os\n",
    "import time\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "# For showing and formatting images\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "\n",
    "# For importing datasets into pytorch\n",
    "import torchvision.datasets as dataset\n",
    "\n",
    "# Used for dataloaders\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# For pretrained resnet34 model\n",
    "import torchvision.models as models\n",
    "\n",
    "# For optimisation function\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# For turning data into tensors\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "# For loss function\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Tensor to wrap data in\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Black-grass  Common Chickweed  Loose Silky-bent   Shepherds Purse\r\n",
      "Charlock     Common wheat      Maize\t\t  Small-flowered Cranesbill\r\n",
      "Cleavers     Fat Hen\t       Scentless Mayweed  Sugar beet\r\n"
     ]
    }
   ],
   "source": [
    "PATH = '/home/cell/data/plant_seedlings/model/'\n",
    "!ls {PATH+\"train\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "sz = 224"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Image loaders\n",
    "## Dataset transforms puts the images in tensor form\n",
    "normalise = transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "train_raw = dataset.ImageFolder(PATH+\"train\", transform=transforms.Compose([transforms.RandomResizedCrop(sz),\n",
    "                                                                            transforms.RandomHorizontalFlip(),\n",
    "                                                                            transforms.ToTensor(),\n",
    "                                                                           normalise]))\n",
    "train_loader = DataLoader(train_raw, batch_size=batch_size, shuffle=True, num_workers=4)\n",
    "\n",
    "valid_raw = dataset.ImageFolder(PATH+\"valid\", transform=transforms.Compose([transforms.CenterCrop(sz),\n",
    "                                                                            transforms.ToTensor(),\n",
    "                                                                           normalise]))\n",
    "valid_loader = DataLoader(valid_raw, batch_size=batch_size, shuffle=False, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(train_raw.classes)\n",
    "#print(train_raw.class_to_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "512\n"
     ]
    }
   ],
   "source": [
    "## Create resnet model\n",
    "resnet34=models.resnet34(pretrained=True)\n",
    "\n",
    "num_ftrs = resnet34.fc.in_features\n",
    "\n",
    "## Freeze all but the last layers\n",
    "for param in resnet34.parameters():\n",
    "    ## Each tensor has the flag requires_grad, setting it to false allows freezes\n",
    "    ## the parmaeter associated with it\n",
    "    param.requires_grad = False\n",
    "    \n",
    "# Parameters of newly constructed modules have requires_grad=True by default\n",
    "## Create new modules that will become final layer\n",
    "num_ftrs = resnet34.fc.in_features\n",
    "print(num_ftrs)\n",
    "## Give final layers a linear transform with twelve outputs one for each category\n",
    "resnet34.fc = nn.Linear(num_ftrs, 13)\n",
    "\n",
    "## Create new model and tell it whether the computer has a GPU or not\n",
    "\n",
    "## Loss function and optimiser\n",
    "criterion = nn.CrossEntropyLoss().cuda()\n",
    "optimiser = optim.Adam(resnet34.fc.parameters(), lr=0.001, weight_decay=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epochs):\n",
    "    #epoch=1\n",
    "    resnet34.train()\n",
    "    time_secs = 0\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        start_time = time.time()\n",
    "        #print(batch_idx)\n",
    "        data, target = Variable(data), Variable(target)\n",
    "        optimiser.zero_grad()\n",
    "        output = resnet34(data)\n",
    "        #print(\"Output: \", output)\n",
    "        #print(\"Target: \", target)\n",
    "        loss=criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimiser.step()\n",
    "        time_secs += (time.time() - start_time)\n",
    "        if batch_idx % 10 == 0:\n",
    "            print(\"Run time for 10 batches was: \", time_secs)\n",
    "            print('Train epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "            epoch, batch_idx*len(data), len(train_loader.dataset),\n",
    "            100. * batch_idx / len(train_loader), loss.data[0]))\n",
    "            time_secs = 0\n",
    "            #break;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation():\n",
    "    resnet34.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    for data, target in valid_loader:\n",
    "        data, target = Variable(data, volatile = True), Variable(target)\n",
    "        output=resnet34(data)\n",
    "        #print(\"output: \", output)\n",
    "        #print(\"target: \", target)\n",
    "        test_loss += criterion(output, target).data[0]\n",
    "        pred = output.data.max(1, keepdim=True)[1]\n",
    "        correct += pred.eq(target.data.view_as(pred)).cpu().sum()\n",
    "    \n",
    "    test_loss /= len(valid_loader.dataset)\n",
    "    \n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "    test_loss, correct, len(valid_loader.dataset),\n",
    "    100. * correct / len(valid_loader.dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run time for 10 batches was:  3.4495320320129395\n",
      "Train epoch: 1 [0/3806 (0%)]\tLoss: 2.774097\n",
      "Run time for 10 batches was:  26.35176110267639\n",
      "Train epoch: 1 [160/3806 (4%)]\tLoss: 2.913017\n",
      "Run time for 10 batches was:  26.201773166656494\n",
      "Train epoch: 1 [320/3806 (8%)]\tLoss: 2.460073\n",
      "Run time for 10 batches was:  26.396099090576172\n",
      "Train epoch: 1 [480/3806 (13%)]\tLoss: 2.263209\n",
      "Run time for 10 batches was:  26.5547878742218\n",
      "Train epoch: 1 [640/3806 (17%)]\tLoss: 1.993510\n",
      "Run time for 10 batches was:  25.955686807632446\n",
      "Train epoch: 1 [800/3806 (21%)]\tLoss: 2.410173\n",
      "Run time for 10 batches was:  26.032048225402832\n",
      "Train epoch: 1 [960/3806 (25%)]\tLoss: 2.123491\n",
      "Run time for 10 batches was:  26.232600927352905\n",
      "Train epoch: 1 [1120/3806 (29%)]\tLoss: 1.959553\n",
      "Run time for 10 batches was:  26.14695405960083\n",
      "Train epoch: 1 [1280/3806 (34%)]\tLoss: 2.104743\n",
      "Run time for 10 batches was:  26.224750995635986\n",
      "Train epoch: 1 [1440/3806 (38%)]\tLoss: 1.782087\n",
      "Run time for 10 batches was:  26.436834573745728\n",
      "Train epoch: 1 [1600/3806 (42%)]\tLoss: 1.758717\n",
      "Run time for 10 batches was:  26.18634057044983\n",
      "Train epoch: 1 [1760/3806 (46%)]\tLoss: 1.758390\n",
      "Run time for 10 batches was:  25.908205032348633\n",
      "Train epoch: 1 [1920/3806 (50%)]\tLoss: 1.868508\n",
      "Run time for 10 batches was:  26.10156226158142\n",
      "Train epoch: 1 [2080/3806 (55%)]\tLoss: 1.802776\n",
      "Run time for 10 batches was:  26.053849697113037\n",
      "Train epoch: 1 [2240/3806 (59%)]\tLoss: 1.921258\n",
      "Run time for 10 batches was:  26.208036184310913\n",
      "Train epoch: 1 [2400/3806 (63%)]\tLoss: 2.411142\n",
      "Run time for 10 batches was:  26.2715163230896\n",
      "Train epoch: 1 [2560/3806 (67%)]\tLoss: 1.707735\n",
      "Run time for 10 batches was:  26.201523780822754\n",
      "Train epoch: 1 [2720/3806 (71%)]\tLoss: 1.390737\n",
      "Run time for 10 batches was:  25.917484521865845\n",
      "Train epoch: 1 [2880/3806 (76%)]\tLoss: 1.849951\n",
      "Run time for 10 batches was:  25.937012910842896\n",
      "Train epoch: 1 [3040/3806 (80%)]\tLoss: 1.528396\n",
      "Run time for 10 batches was:  26.158427000045776\n",
      "Train epoch: 1 [3200/3806 (84%)]\tLoss: 1.910473\n",
      "Run time for 10 batches was:  25.922225952148438\n",
      "Train epoch: 1 [3360/3806 (88%)]\tLoss: 1.774712\n",
      "Run time for 10 batches was:  26.150463581085205\n",
      "Train epoch: 1 [3520/3806 (92%)]\tLoss: 1.858042\n",
      "Run time for 10 batches was:  26.065340757369995\n",
      "Train epoch: 1 [3680/3806 (97%)]\tLoss: 1.656062\n",
      "\n",
      "Test set: Average loss: 0.1079, Accuracy: 435/944 (46%)\n",
      "\n",
      "Run time for 10 batches was:  3.0104684829711914\n",
      "Train epoch: 2 [0/3806 (0%)]\tLoss: 1.468406\n",
      "Run time for 10 batches was:  26.006375074386597\n",
      "Train epoch: 2 [160/3806 (4%)]\tLoss: 1.864961\n",
      "Run time for 10 batches was:  26.060643911361694\n",
      "Train epoch: 2 [320/3806 (8%)]\tLoss: 1.363400\n",
      "Run time for 10 batches was:  25.74155831336975\n",
      "Train epoch: 2 [480/3806 (13%)]\tLoss: 1.682209\n",
      "Run time for 10 batches was:  25.80126428604126\n",
      "Train epoch: 2 [640/3806 (17%)]\tLoss: 1.602355\n",
      "Run time for 10 batches was:  25.803677558898926\n",
      "Train epoch: 2 [800/3806 (21%)]\tLoss: 1.050700\n",
      "Run time for 10 batches was:  25.94876718521118\n",
      "Train epoch: 2 [960/3806 (25%)]\tLoss: 1.254771\n",
      "Run time for 10 batches was:  26.03580093383789\n",
      "Train epoch: 2 [1120/3806 (29%)]\tLoss: 1.060663\n",
      "Run time for 10 batches was:  25.990902423858643\n",
      "Train epoch: 2 [1280/3806 (34%)]\tLoss: 1.818206\n",
      "Run time for 10 batches was:  25.858521938323975\n",
      "Train epoch: 2 [1440/3806 (38%)]\tLoss: 1.626192\n",
      "Run time for 10 batches was:  26.005252838134766\n",
      "Train epoch: 2 [1600/3806 (42%)]\tLoss: 1.367497\n",
      "Run time for 10 batches was:  25.93142294883728\n",
      "Train epoch: 2 [1760/3806 (46%)]\tLoss: 0.876770\n",
      "Run time for 10 batches was:  25.918750286102295\n",
      "Train epoch: 2 [1920/3806 (50%)]\tLoss: 1.187293\n",
      "Run time for 10 batches was:  25.89910578727722\n",
      "Train epoch: 2 [2080/3806 (55%)]\tLoss: 1.256223\n",
      "Run time for 10 batches was:  25.984282970428467\n",
      "Train epoch: 2 [2240/3806 (59%)]\tLoss: 1.052411\n",
      "Run time for 10 batches was:  25.70614218711853\n",
      "Train epoch: 2 [2400/3806 (63%)]\tLoss: 1.576744\n",
      "Run time for 10 batches was:  25.96895980834961\n",
      "Train epoch: 2 [2560/3806 (67%)]\tLoss: 0.928932\n",
      "Run time for 10 batches was:  25.75456929206848\n",
      "Train epoch: 2 [2720/3806 (71%)]\tLoss: 1.019585\n",
      "Run time for 10 batches was:  25.74879026412964\n",
      "Train epoch: 2 [2880/3806 (76%)]\tLoss: 1.480973\n",
      "Run time for 10 batches was:  26.06178903579712\n",
      "Train epoch: 2 [3040/3806 (80%)]\tLoss: 1.540569\n",
      "Run time for 10 batches was:  25.989611625671387\n",
      "Train epoch: 2 [3200/3806 (84%)]\tLoss: 1.247216\n",
      "Run time for 10 batches was:  25.984267234802246\n",
      "Train epoch: 2 [3360/3806 (88%)]\tLoss: 1.149861\n",
      "Run time for 10 batches was:  25.878272771835327\n",
      "Train epoch: 2 [3520/3806 (92%)]\tLoss: 1.258906\n",
      "Run time for 10 batches was:  25.7108211517334\n",
      "Train epoch: 2 [3680/3806 (97%)]\tLoss: 2.072905\n",
      "\n",
      "Test set: Average loss: 0.0969, Accuracy: 472/944 (50%)\n",
      "\n",
      "Run time for 10 batches was:  2.8231773376464844\n",
      "Train epoch: 3 [0/3806 (0%)]\tLoss: 1.284775\n",
      "Run time for 10 batches was:  25.807981252670288\n",
      "Train epoch: 3 [160/3806 (4%)]\tLoss: 1.278231\n",
      "Run time for 10 batches was:  25.88829469680786\n",
      "Train epoch: 3 [320/3806 (8%)]\tLoss: 1.177577\n",
      "Run time for 10 batches was:  25.805335998535156\n",
      "Train epoch: 3 [480/3806 (13%)]\tLoss: 1.451057\n",
      "Run time for 10 batches was:  26.07539463043213\n",
      "Train epoch: 3 [640/3806 (17%)]\tLoss: 1.254190\n",
      "Run time for 10 batches was:  25.858612060546875\n",
      "Train epoch: 3 [800/3806 (21%)]\tLoss: 0.918744\n",
      "Run time for 10 batches was:  26.02183437347412\n",
      "Train epoch: 3 [960/3806 (25%)]\tLoss: 1.437420\n",
      "Run time for 10 batches was:  25.892525911331177\n",
      "Train epoch: 3 [1120/3806 (29%)]\tLoss: 1.401699\n",
      "Run time for 10 batches was:  26.082597017288208\n",
      "Train epoch: 3 [1280/3806 (34%)]\tLoss: 1.302863\n",
      "Run time for 10 batches was:  26.033637046813965\n",
      "Train epoch: 3 [1440/3806 (38%)]\tLoss: 1.208710\n",
      "Run time for 10 batches was:  25.677728176116943\n",
      "Train epoch: 3 [1600/3806 (42%)]\tLoss: 1.436354\n",
      "Run time for 10 batches was:  26.12842631340027\n",
      "Train epoch: 3 [1760/3806 (46%)]\tLoss: 1.488195\n",
      "Run time for 10 batches was:  26.06698989868164\n",
      "Train epoch: 3 [1920/3806 (50%)]\tLoss: 1.125254\n",
      "Run time for 10 batches was:  25.962191820144653\n",
      "Train epoch: 3 [2080/3806 (55%)]\tLoss: 1.484827\n",
      "Run time for 10 batches was:  25.81160616874695\n",
      "Train epoch: 3 [2240/3806 (59%)]\tLoss: 1.323465\n",
      "Run time for 10 batches was:  25.9678955078125\n",
      "Train epoch: 3 [2400/3806 (63%)]\tLoss: 0.995022\n",
      "Run time for 10 batches was:  25.817089080810547\n",
      "Train epoch: 3 [2560/3806 (67%)]\tLoss: 0.955541\n",
      "Run time for 10 batches was:  25.737186670303345\n",
      "Train epoch: 3 [2720/3806 (71%)]\tLoss: 0.785529\n",
      "Run time for 10 batches was:  25.794835805892944\n",
      "Train epoch: 3 [2880/3806 (76%)]\tLoss: 1.503687\n",
      "Run time for 10 batches was:  26.147138595581055\n",
      "Train epoch: 3 [3040/3806 (80%)]\tLoss: 1.285422\n",
      "Run time for 10 batches was:  25.962013483047485\n",
      "Train epoch: 3 [3200/3806 (84%)]\tLoss: 1.469298\n",
      "Run time for 10 batches was:  26.434925079345703\n",
      "Train epoch: 3 [3360/3806 (88%)]\tLoss: 1.544456\n",
      "Run time for 10 batches was:  25.85657525062561\n",
      "Train epoch: 3 [3520/3806 (92%)]\tLoss: 0.748664\n",
      "Run time for 10 batches was:  25.906598806381226\n",
      "Train epoch: 3 [3680/3806 (97%)]\tLoss: 1.493857\n",
      "\n",
      "Test set: Average loss: 0.0976, Accuracy: 477/944 (51%)\n",
      "\n",
      "Run time for 10 batches was:  3.0182881355285645\n",
      "Train epoch: 4 [0/3806 (0%)]\tLoss: 1.158610\n",
      "Run time for 10 batches was:  25.90777015686035\n",
      "Train epoch: 4 [160/3806 (4%)]\tLoss: 1.175317\n",
      "Run time for 10 batches was:  25.930983066558838\n",
      "Train epoch: 4 [320/3806 (8%)]\tLoss: 1.596285\n",
      "Run time for 10 batches was:  25.66973638534546\n",
      "Train epoch: 4 [480/3806 (13%)]\tLoss: 0.979852\n",
      "Run time for 10 batches was:  26.0536687374115\n",
      "Train epoch: 4 [640/3806 (17%)]\tLoss: 1.279378\n",
      "Run time for 10 batches was:  26.0000159740448\n",
      "Train epoch: 4 [800/3806 (21%)]\tLoss: 1.100194\n",
      "Run time for 10 batches was:  26.01162362098694\n",
      "Train epoch: 4 [960/3806 (25%)]\tLoss: 1.184853\n",
      "Run time for 10 batches was:  26.02949047088623\n",
      "Train epoch: 4 [1120/3806 (29%)]\tLoss: 1.024345\n",
      "Run time for 10 batches was:  25.97848343849182\n",
      "Train epoch: 4 [1280/3806 (34%)]\tLoss: 1.504612\n",
      "Run time for 10 batches was:  25.882158994674683\n",
      "Train epoch: 4 [1440/3806 (38%)]\tLoss: 0.996317\n",
      "Run time for 10 batches was:  25.84770894050598\n",
      "Train epoch: 4 [1600/3806 (42%)]\tLoss: 1.205267\n",
      "Run time for 10 batches was:  25.968551874160767\n",
      "Train epoch: 4 [1760/3806 (46%)]\tLoss: 1.223290\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run time for 10 batches was:  25.771840572357178\n",
      "Train epoch: 4 [1920/3806 (50%)]\tLoss: 1.063787\n",
      "Run time for 10 batches was:  26.04831027984619\n",
      "Train epoch: 4 [2080/3806 (55%)]\tLoss: 1.201853\n",
      "Run time for 10 batches was:  25.639350652694702\n",
      "Train epoch: 4 [2240/3806 (59%)]\tLoss: 1.365049\n",
      "Run time for 10 batches was:  25.76971125602722\n",
      "Train epoch: 4 [2400/3806 (63%)]\tLoss: 0.682599\n",
      "Run time for 10 batches was:  25.950114965438843\n",
      "Train epoch: 4 [2560/3806 (67%)]\tLoss: 1.086460\n",
      "Run time for 10 batches was:  25.744615077972412\n",
      "Train epoch: 4 [2720/3806 (71%)]\tLoss: 0.895966\n",
      "Run time for 10 batches was:  25.754014253616333\n",
      "Train epoch: 4 [2880/3806 (76%)]\tLoss: 1.333724\n",
      "Run time for 10 batches was:  25.76490068435669\n",
      "Train epoch: 4 [3040/3806 (80%)]\tLoss: 1.164353\n",
      "Run time for 10 batches was:  26.095804691314697\n",
      "Train epoch: 4 [3200/3806 (84%)]\tLoss: 1.468507\n",
      "Run time for 10 batches was:  26.097576379776\n",
      "Train epoch: 4 [3360/3806 (88%)]\tLoss: 1.068233\n",
      "Run time for 10 batches was:  25.877159357070923\n",
      "Train epoch: 4 [3520/3806 (92%)]\tLoss: 1.194327\n",
      "Run time for 10 batches was:  25.941077709197998\n",
      "Train epoch: 4 [3680/3806 (97%)]\tLoss: 0.974901\n",
      "\n",
      "Test set: Average loss: 0.0964, Accuracy: 429/944 (45%)\n",
      "\n",
      "Run time for 10 batches was:  2.961200714111328\n",
      "Train epoch: 5 [0/3806 (0%)]\tLoss: 0.942846\n",
      "Run time for 10 batches was:  26.237587451934814\n",
      "Train epoch: 5 [160/3806 (4%)]\tLoss: 1.144367\n",
      "Run time for 10 batches was:  25.75121521949768\n",
      "Train epoch: 5 [320/3806 (8%)]\tLoss: 1.201791\n",
      "Run time for 10 batches was:  25.811062335968018\n",
      "Train epoch: 5 [480/3806 (13%)]\tLoss: 0.741229\n",
      "Run time for 10 batches was:  25.662964820861816\n",
      "Train epoch: 5 [640/3806 (17%)]\tLoss: 0.943370\n",
      "Run time for 10 batches was:  26.120578289031982\n",
      "Train epoch: 5 [800/3806 (21%)]\tLoss: 1.248412\n",
      "Run time for 10 batches was:  25.982852935791016\n",
      "Train epoch: 5 [960/3806 (25%)]\tLoss: 0.891349\n",
      "Run time for 10 batches was:  25.926623582839966\n",
      "Train epoch: 5 [1120/3806 (29%)]\tLoss: 0.916364\n",
      "Run time for 10 batches was:  26.079869031906128\n",
      "Train epoch: 5 [1280/3806 (34%)]\tLoss: 0.941317\n",
      "Run time for 10 batches was:  25.890791654586792\n",
      "Train epoch: 5 [1440/3806 (38%)]\tLoss: 1.321304\n",
      "Run time for 10 batches was:  25.978474617004395\n",
      "Train epoch: 5 [1600/3806 (42%)]\tLoss: 1.128436\n",
      "Run time for 10 batches was:  25.93259072303772\n",
      "Train epoch: 5 [1760/3806 (46%)]\tLoss: 1.396509\n",
      "Run time for 10 batches was:  26.13203740119934\n",
      "Train epoch: 5 [1920/3806 (50%)]\tLoss: 1.319341\n",
      "Run time for 10 batches was:  25.798992395401\n",
      "Train epoch: 5 [2080/3806 (55%)]\tLoss: 1.134040\n",
      "Run time for 10 batches was:  26.53227210044861\n",
      "Train epoch: 5 [2240/3806 (59%)]\tLoss: 0.878362\n",
      "Run time for 10 batches was:  25.976333379745483\n",
      "Train epoch: 5 [2400/3806 (63%)]\tLoss: 1.534711\n",
      "Run time for 10 batches was:  26.1042001247406\n",
      "Train epoch: 5 [2560/3806 (67%)]\tLoss: 0.845980\n",
      "Run time for 10 batches was:  25.88312530517578\n",
      "Train epoch: 5 [2720/3806 (71%)]\tLoss: 0.954727\n",
      "Run time for 10 batches was:  25.98753595352173\n",
      "Train epoch: 5 [2880/3806 (76%)]\tLoss: 0.978092\n",
      "Run time for 10 batches was:  25.980380058288574\n",
      "Train epoch: 5 [3040/3806 (80%)]\tLoss: 1.038422\n",
      "Run time for 10 batches was:  26.094990015029907\n",
      "Train epoch: 5 [3200/3806 (84%)]\tLoss: 1.362131\n",
      "Run time for 10 batches was:  25.78532838821411\n",
      "Train epoch: 5 [3360/3806 (88%)]\tLoss: 1.027663\n",
      "Run time for 10 batches was:  25.843441009521484\n",
      "Train epoch: 5 [3520/3806 (92%)]\tLoss: 1.084912\n",
      "Run time for 10 batches was:  25.98634934425354\n",
      "Train epoch: 5 [3680/3806 (97%)]\tLoss: 1.234207\n",
      "\n",
      "Test set: Average loss: 0.0902, Accuracy: 491/944 (52%)\n",
      "\n",
      "Run time for 10 batches was:  2.903156280517578\n",
      "Train epoch: 6 [0/3806 (0%)]\tLoss: 1.031547\n",
      "Run time for 10 batches was:  25.920512914657593\n",
      "Train epoch: 6 [160/3806 (4%)]\tLoss: 0.857687\n",
      "Run time for 10 batches was:  25.93739342689514\n",
      "Train epoch: 6 [320/3806 (8%)]\tLoss: 1.361138\n",
      "Run time for 10 batches was:  25.814328908920288\n",
      "Train epoch: 6 [480/3806 (13%)]\tLoss: 0.940987\n",
      "Run time for 10 batches was:  25.93498396873474\n",
      "Train epoch: 6 [640/3806 (17%)]\tLoss: 0.755141\n",
      "Run time for 10 batches was:  26.008219957351685\n",
      "Train epoch: 6 [800/3806 (21%)]\tLoss: 1.106000\n",
      "Run time for 10 batches was:  25.5590980052948\n",
      "Train epoch: 6 [960/3806 (25%)]\tLoss: 1.281655\n",
      "Run time for 10 batches was:  25.99705147743225\n",
      "Train epoch: 6 [1120/3806 (29%)]\tLoss: 1.385938\n",
      "Run time for 10 batches was:  26.07799744606018\n",
      "Train epoch: 6 [1280/3806 (34%)]\tLoss: 0.846287\n",
      "Run time for 10 batches was:  25.893012046813965\n",
      "Train epoch: 6 [1440/3806 (38%)]\tLoss: 0.985887\n",
      "Run time for 10 batches was:  26.169021129608154\n",
      "Train epoch: 6 [1600/3806 (42%)]\tLoss: 1.376252\n",
      "Run time for 10 batches was:  25.883394479751587\n",
      "Train epoch: 6 [1760/3806 (46%)]\tLoss: 1.280823\n",
      "Run time for 10 batches was:  26.10344433784485\n",
      "Train epoch: 6 [1920/3806 (50%)]\tLoss: 0.748263\n",
      "Run time for 10 batches was:  26.316248416900635\n",
      "Train epoch: 6 [2080/3806 (55%)]\tLoss: 0.606298\n",
      "Run time for 10 batches was:  25.8957839012146\n",
      "Train epoch: 6 [2240/3806 (59%)]\tLoss: 1.131213\n",
      "Run time for 10 batches was:  25.920849323272705\n",
      "Train epoch: 6 [2400/3806 (63%)]\tLoss: 1.123856\n",
      "Run time for 10 batches was:  25.884565591812134\n",
      "Train epoch: 6 [2560/3806 (67%)]\tLoss: 0.612163\n",
      "Run time for 10 batches was:  25.940768718719482\n",
      "Train epoch: 6 [2720/3806 (71%)]\tLoss: 1.267230\n",
      "Run time for 10 batches was:  26.148094415664673\n",
      "Train epoch: 6 [2880/3806 (76%)]\tLoss: 1.094110\n",
      "Run time for 10 batches was:  25.90546679496765\n",
      "Train epoch: 6 [3040/3806 (80%)]\tLoss: 1.016082\n",
      "Run time for 10 batches was:  26.153512954711914\n",
      "Train epoch: 6 [3200/3806 (84%)]\tLoss: 1.245131\n",
      "Run time for 10 batches was:  25.956881761550903\n",
      "Train epoch: 6 [3360/3806 (88%)]\tLoss: 1.143107\n",
      "Run time for 10 batches was:  25.941365242004395\n",
      "Train epoch: 6 [3520/3806 (92%)]\tLoss: 1.089863\n",
      "Run time for 10 batches was:  26.255767107009888\n",
      "Train epoch: 6 [3680/3806 (97%)]\tLoss: 1.074438\n",
      "\n",
      "Test set: Average loss: 0.0918, Accuracy: 477/944 (51%)\n",
      "\n",
      "Run time for 10 batches was:  2.753579616546631\n",
      "Train epoch: 7 [0/3806 (0%)]\tLoss: 1.390688\n",
      "Run time for 10 batches was:  25.902621269226074\n",
      "Train epoch: 7 [160/3806 (4%)]\tLoss: 0.809139\n",
      "Run time for 10 batches was:  26.22254467010498\n",
      "Train epoch: 7 [320/3806 (8%)]\tLoss: 0.998658\n",
      "Run time for 10 batches was:  25.926586866378784\n",
      "Train epoch: 7 [480/3806 (13%)]\tLoss: 1.412616\n",
      "Run time for 10 batches was:  25.78664207458496\n",
      "Train epoch: 7 [640/3806 (17%)]\tLoss: 1.129506\n",
      "Run time for 10 batches was:  26.42481255531311\n",
      "Train epoch: 7 [800/3806 (21%)]\tLoss: 1.142533\n",
      "Run time for 10 batches was:  25.93522357940674\n",
      "Train epoch: 7 [960/3806 (25%)]\tLoss: 0.834444\n",
      "Run time for 10 batches was:  25.758769035339355\n",
      "Train epoch: 7 [1120/3806 (29%)]\tLoss: 0.903710\n",
      "Run time for 10 batches was:  25.881274461746216\n",
      "Train epoch: 7 [1280/3806 (34%)]\tLoss: 0.801801\n",
      "Run time for 10 batches was:  25.906115770339966\n",
      "Train epoch: 7 [1440/3806 (38%)]\tLoss: 1.371018\n",
      "Run time for 10 batches was:  25.87613320350647\n",
      "Train epoch: 7 [1600/3806 (42%)]\tLoss: 1.225683\n",
      "Run time for 10 batches was:  25.996825218200684\n",
      "Train epoch: 7 [1760/3806 (46%)]\tLoss: 0.845231\n",
      "Run time for 10 batches was:  25.867806911468506\n",
      "Train epoch: 7 [1920/3806 (50%)]\tLoss: 0.698438\n",
      "Run time for 10 batches was:  25.709856033325195\n",
      "Train epoch: 7 [2080/3806 (55%)]\tLoss: 1.186672\n",
      "Run time for 10 batches was:  26.057404041290283\n",
      "Train epoch: 7 [2240/3806 (59%)]\tLoss: 0.868855\n",
      "Run time for 10 batches was:  26.0934796333313\n",
      "Train epoch: 7 [2400/3806 (63%)]\tLoss: 1.067319\n",
      "Run time for 10 batches was:  26.366869688034058\n",
      "Train epoch: 7 [2560/3806 (67%)]\tLoss: 1.132573\n",
      "Run time for 10 batches was:  26.061153411865234\n",
      "Train epoch: 7 [2720/3806 (71%)]\tLoss: 1.146435\n",
      "Run time for 10 batches was:  25.85331654548645\n",
      "Train epoch: 7 [2880/3806 (76%)]\tLoss: 1.437982\n",
      "Run time for 10 batches was:  26.14556574821472\n",
      "Train epoch: 7 [3040/3806 (80%)]\tLoss: 0.831933\n",
      "Run time for 10 batches was:  25.736923933029175\n",
      "Train epoch: 7 [3200/3806 (84%)]\tLoss: 1.068606\n",
      "Run time for 10 batches was:  25.94663691520691\n",
      "Train epoch: 7 [3360/3806 (88%)]\tLoss: 0.805790\n",
      "Run time for 10 batches was:  26.133092403411865\n",
      "Train epoch: 7 [3520/3806 (92%)]\tLoss: 1.233147\n",
      "Run time for 10 batches was:  25.876712799072266\n",
      "Train epoch: 7 [3680/3806 (97%)]\tLoss: 0.763979\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 0.0942, Accuracy: 451/944 (48%)\n",
      "\n",
      "Run time for 10 batches was:  2.8943941593170166\n",
      "Train epoch: 8 [0/3806 (0%)]\tLoss: 1.073682\n",
      "Run time for 10 batches was:  25.732027053833008\n",
      "Train epoch: 8 [160/3806 (4%)]\tLoss: 0.996754\n",
      "Run time for 10 batches was:  25.954986095428467\n",
      "Train epoch: 8 [320/3806 (8%)]\tLoss: 0.954607\n",
      "Run time for 10 batches was:  25.840912580490112\n",
      "Train epoch: 8 [480/3806 (13%)]\tLoss: 1.496408\n",
      "Run time for 10 batches was:  26.149842023849487\n",
      "Train epoch: 8 [640/3806 (17%)]\tLoss: 1.090403\n",
      "Run time for 10 batches was:  25.899043798446655\n",
      "Train epoch: 8 [800/3806 (21%)]\tLoss: 0.940197\n",
      "Run time for 10 batches was:  25.7549090385437\n",
      "Train epoch: 8 [960/3806 (25%)]\tLoss: 1.046291\n",
      "Run time for 10 batches was:  26.162492275238037\n",
      "Train epoch: 8 [1120/3806 (29%)]\tLoss: 1.261951\n",
      "Run time for 10 batches was:  25.876591205596924\n",
      "Train epoch: 8 [1280/3806 (34%)]\tLoss: 1.148438\n",
      "Run time for 10 batches was:  25.98087239265442\n",
      "Train epoch: 8 [1440/3806 (38%)]\tLoss: 0.582565\n",
      "Run time for 10 batches was:  25.718466758728027\n",
      "Train epoch: 8 [1600/3806 (42%)]\tLoss: 0.608913\n",
      "Run time for 10 batches was:  26.005438327789307\n",
      "Train epoch: 8 [1760/3806 (46%)]\tLoss: 1.422353\n",
      "Run time for 10 batches was:  26.15531325340271\n",
      "Train epoch: 8 [1920/3806 (50%)]\tLoss: 0.791019\n",
      "Run time for 10 batches was:  26.052658557891846\n",
      "Train epoch: 8 [2080/3806 (55%)]\tLoss: 0.718931\n",
      "Run time for 10 batches was:  26.047983407974243\n",
      "Train epoch: 8 [2240/3806 (59%)]\tLoss: 0.920361\n",
      "Run time for 10 batches was:  26.08758521080017\n",
      "Train epoch: 8 [2400/3806 (63%)]\tLoss: 1.385501\n",
      "Run time for 10 batches was:  25.705054759979248\n",
      "Train epoch: 8 [2560/3806 (67%)]\tLoss: 0.867085\n",
      "Run time for 10 batches was:  26.065975427627563\n",
      "Train epoch: 8 [2720/3806 (71%)]\tLoss: 0.987351\n",
      "Run time for 10 batches was:  26.10355806350708\n",
      "Train epoch: 8 [2880/3806 (76%)]\tLoss: 0.865036\n",
      "Run time for 10 batches was:  25.932870388031006\n",
      "Train epoch: 8 [3040/3806 (80%)]\tLoss: 0.869709\n",
      "Run time for 10 batches was:  26.07295036315918\n",
      "Train epoch: 8 [3200/3806 (84%)]\tLoss: 1.013353\n",
      "Run time for 10 batches was:  26.33029866218567\n",
      "Train epoch: 8 [3360/3806 (88%)]\tLoss: 0.969081\n",
      "Run time for 10 batches was:  25.794825553894043\n",
      "Train epoch: 8 [3520/3806 (92%)]\tLoss: 1.166638\n",
      "Run time for 10 batches was:  25.779070615768433\n",
      "Train epoch: 8 [3680/3806 (97%)]\tLoss: 0.662318\n",
      "\n",
      "Test set: Average loss: 0.0959, Accuracy: 456/944 (48%)\n",
      "\n",
      "Run time for 10 batches was:  2.9181478023529053\n",
      "Train epoch: 9 [0/3806 (0%)]\tLoss: 0.783092\n",
      "Run time for 10 batches was:  26.116705179214478\n",
      "Train epoch: 9 [160/3806 (4%)]\tLoss: 1.163441\n",
      "Run time for 10 batches was:  26.011387586593628\n",
      "Train epoch: 9 [320/3806 (8%)]\tLoss: 1.088169\n",
      "Run time for 10 batches was:  25.869974613189697\n",
      "Train epoch: 9 [480/3806 (13%)]\tLoss: 0.946448\n",
      "Run time for 10 batches was:  25.69386076927185\n",
      "Train epoch: 9 [640/3806 (17%)]\tLoss: 1.180081\n",
      "Run time for 10 batches was:  25.702910900115967\n",
      "Train epoch: 9 [800/3806 (21%)]\tLoss: 1.030101\n",
      "Run time for 10 batches was:  25.982398748397827\n",
      "Train epoch: 9 [960/3806 (25%)]\tLoss: 0.928063\n",
      "Run time for 10 batches was:  25.96263837814331\n",
      "Train epoch: 9 [1120/3806 (29%)]\tLoss: 0.909935\n",
      "Run time for 10 batches was:  25.830884218215942\n",
      "Train epoch: 9 [1280/3806 (34%)]\tLoss: 0.991108\n",
      "Run time for 10 batches was:  26.11010456085205\n",
      "Train epoch: 9 [1440/3806 (38%)]\tLoss: 0.901582\n",
      "Run time for 10 batches was:  26.233877897262573\n",
      "Train epoch: 9 [1600/3806 (42%)]\tLoss: 1.001944\n",
      "Run time for 10 batches was:  25.7195827960968\n",
      "Train epoch: 9 [1760/3806 (46%)]\tLoss: 0.685116\n",
      "Run time for 10 batches was:  25.78716540336609\n",
      "Train epoch: 9 [1920/3806 (50%)]\tLoss: 0.866522\n",
      "Run time for 10 batches was:  26.148327112197876\n",
      "Train epoch: 9 [2080/3806 (55%)]\tLoss: 1.084651\n",
      "Run time for 10 batches was:  26.150280714035034\n",
      "Train epoch: 9 [2240/3806 (59%)]\tLoss: 1.023688\n",
      "Run time for 10 batches was:  26.135634899139404\n",
      "Train epoch: 9 [2400/3806 (63%)]\tLoss: 0.880153\n",
      "Run time for 10 batches was:  25.90007519721985\n",
      "Train epoch: 9 [2560/3806 (67%)]\tLoss: 1.744503\n",
      "Run time for 10 batches was:  25.709882974624634\n",
      "Train epoch: 9 [2720/3806 (71%)]\tLoss: 1.006899\n",
      "Run time for 10 batches was:  25.978749752044678\n",
      "Train epoch: 9 [2880/3806 (76%)]\tLoss: 1.017539\n",
      "Run time for 10 batches was:  25.90349245071411\n",
      "Train epoch: 9 [3040/3806 (80%)]\tLoss: 1.120749\n",
      "Run time for 10 batches was:  25.825355768203735\n",
      "Train epoch: 9 [3200/3806 (84%)]\tLoss: 1.363537\n",
      "Run time for 10 batches was:  25.836215496063232\n",
      "Train epoch: 9 [3360/3806 (88%)]\tLoss: 1.017271\n",
      "Run time for 10 batches was:  26.03857183456421\n",
      "Train epoch: 9 [3520/3806 (92%)]\tLoss: 0.927392\n",
      "Run time for 10 batches was:  25.95535659790039\n",
      "Train epoch: 9 [3680/3806 (97%)]\tLoss: 1.232277\n",
      "\n",
      "Test set: Average loss: 0.0931, Accuracy: 471/944 (50%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## Loop through epochs training data and then testing it\n",
    "for epoch in range(1,10):\n",
    "    train(epoch)\n",
    "    validation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
